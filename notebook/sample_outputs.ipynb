{
 "cells": [
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "markdown",
   "source": [
    "# Sample Outputs – Generative AI RAG (Part 1)\n",
    "\n",
    "This section shows sample outputs from my Part 1 pipeline:\n",
    "- chunking stats\n",
    "- embedding dimension check\n",
    "- ChromaDB retrieval examples (top-k neighbors + distances)\n",
    "\n",
    "Dataset is not included in the repo. See README for download and file path."
   ],
   "id": "e99eb5f19ef98543"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Note:**\n",
    "This notebook expects an OpenAI API key to be provided either via the\n",
    "`OPENAI_API_KEY` environment variable or via a local `openai.txt` file\n",
    "(not included in the repository)."
   ],
   "id": "b532803f2a49bebb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 1) Environment check",
   "id": "5a0c6e090e316df5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-04T10:22:21.608862Z",
     "start_time": "2026-02-04T10:22:21.605505Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import sys, chromadb, pandas as pd\n",
    "import openai\n",
    "\n",
    "print(\"Python:\", sys.version)\n",
    "print(\"chromadb:\", chromadb.__version__)\n",
    "print(\"pandas:\", pd.__version__)\n",
    "print(\"openai:\", openai.__version__)"
   ],
   "id": "a92215a2766b518c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.13.7 (v3.13.7:bcee1c32211, Aug 14 2025, 19:10:51) [Clang 16.0.0 (clang-1600.0.26.6)]\n",
      "chromadb: 1.4.1\n",
      "pandas: 3.0.0\n",
      "openai: 2.16.0\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2) Load dataset + show basic info",
   "id": "a0b7152dfc60c99e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-04T10:22:21.641588Z",
     "start_time": "2026-02-04T10:22:21.625313Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "DATA_PATH = Path(\"../data/ai_agents_jobs/AI_Agents_Ecosystem_2026.csv\")\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "print(\"Shape:\", df.shape)\n",
    "print(\"Columns:\", list(df.columns))\n",
    "df.head(2)"
   ],
   "id": "efb08fb1aeec130a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (1206, 5)\n",
      "Columns: ['Title', 'Source', 'Date', 'Description', 'Link']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "                                               Title     Source        Date  \\\n",
       "0      Client Support Specialist at Clipboard Health  RemoteJob  2026-01-16   \n",
       "1  Senior Independent AI Engineer / Architect at ...  RemoteJob  2026-01-16   \n",
       "\n",
       "                                         Description  \\\n",
       "0  About the Role\\n \\nClipboard Health is looking...   \n",
       "1  Location: Americas, Europe, or Israel\\nThe Opp...   \n",
       "\n",
       "                                                Link  \n",
       "0  https://remotive.com/remote-jobs/customer-serv...  \n",
       "1  https://remotive.com/remote-jobs/software-deve...  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Source</th>\n",
       "      <th>Date</th>\n",
       "      <th>Description</th>\n",
       "      <th>Link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Client Support Specialist at Clipboard Health</td>\n",
       "      <td>RemoteJob</td>\n",
       "      <td>2026-01-16</td>\n",
       "      <td>About the Role\\n \\nClipboard Health is looking...</td>\n",
       "      <td>https://remotive.com/remote-jobs/customer-serv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Senior Independent AI Engineer / Architect at ...</td>\n",
       "      <td>RemoteJob</td>\n",
       "      <td>2026-01-16</td>\n",
       "      <td>Location: Americas, Europe, or Israel\\nThe Opp...</td>\n",
       "      <td>https://remotive.com/remote-jobs/software-deve...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3) Out-of-scope filter evidence (post–Oct 2023)",
   "id": "2164de8836bb4672"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-04T10:22:21.677657Z",
     "start_time": "2026-02-04T10:22:21.667094Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df2 = df.copy()\n",
    "df2[\"Date\"] = pd.to_datetime(df2[\"Date\"], errors=\"coerce\")\n",
    "\n",
    "cutoff = pd.Timestamp(\"2023-10-01\")\n",
    "post_cutoff = df2[df2[\"Date\"] >= cutoff]\n",
    "\n",
    "print(\"Full date range:\", df2[\"Date\"].min(), \"→\", df2[\"Date\"].max())\n",
    "print(\"Rows post-cutoff (>= 2023-10-01):\", post_cutoff.shape[0], \"out of\", df2.shape[0])\n",
    "post_cutoff[[\"Title\", \"Source\", \"Date\", \"Link\"]].head(5)"
   ],
   "id": "c738596d3b072c1f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full date range: 2009-07-01 00:00:00 → 2026-01-16 00:00:00\n",
      "Rows post-cutoff (>= 2023-10-01): 885 out of 1206\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "                                               Title      Source       Date  \\\n",
       "0      Client Support Specialist at Clipboard Health   RemoteJob 2026-01-16   \n",
       "1  Senior Independent AI Engineer / Architect at ...   RemoteJob 2026-01-16   \n",
       "2    Senior Independent Software Developer at A.Team   RemoteJob 2026-01-16   \n",
       "3  Show HN: Gambit, an open-source agent harness ...  HackerNews 2026-01-16   \n",
       "4  Show HN: Use-AI: trivially add AI automation t...  HackerNews 2026-01-16   \n",
       "\n",
       "                                                Link  \n",
       "0  https://remotive.com/remote-jobs/customer-serv...  \n",
       "1  https://remotive.com/remote-jobs/software-deve...  \n",
       "2  https://remotive.com/remote-jobs/software-deve...  \n",
       "3             https://github.com/bolt-foundry/gambit  \n",
       "4                https://github.com/meetsmore/use-ai  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Source</th>\n",
       "      <th>Date</th>\n",
       "      <th>Link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Client Support Specialist at Clipboard Health</td>\n",
       "      <td>RemoteJob</td>\n",
       "      <td>2026-01-16</td>\n",
       "      <td>https://remotive.com/remote-jobs/customer-serv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Senior Independent AI Engineer / Architect at ...</td>\n",
       "      <td>RemoteJob</td>\n",
       "      <td>2026-01-16</td>\n",
       "      <td>https://remotive.com/remote-jobs/software-deve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Senior Independent Software Developer at A.Team</td>\n",
       "      <td>RemoteJob</td>\n",
       "      <td>2026-01-16</td>\n",
       "      <td>https://remotive.com/remote-jobs/software-deve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Show HN: Gambit, an open-source agent harness ...</td>\n",
       "      <td>HackerNews</td>\n",
       "      <td>2026-01-16</td>\n",
       "      <td>https://github.com/bolt-foundry/gambit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Show HN: Use-AI: trivially add AI automation t...</td>\n",
       "      <td>HackerNews</td>\n",
       "      <td>2026-01-16</td>\n",
       "      <td>https://github.com/meetsmore/use-ai</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 4) Chunking experiment output (Markdown + code cell)\n",
    "sample code from chunk_smoketest.py.\n",
    "\n",
    "Reproduce the key stats:\n",
    "* total rows\n",
    "* total chunks\n",
    "* avg chunks/row for 2–3 configs\n",
    "* one example chunk + metadata"
   ],
   "id": "d8c8bc7579c09660"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-04T10:22:21.948869Z",
     "start_time": "2026-02-04T10:22:21.918685Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "df3 = df2.copy()\n",
    "df3[\"Description\"] = df3[\"Description\"].fillna(\"\").astype(str)\n",
    "\n",
    "def row_to_doc(row) -> str:\n",
    "    return \"\\n\".join([\n",
    "        f\"TITLE: {row.get('Title','')}\",\n",
    "        f\"SOURCE: {row.get('Source','')}\",\n",
    "        f\"DATE: {row.get('Date','')}\",\n",
    "        f\"DESCRIPTION: {row.get('Description','')}\",\n",
    "    ])\n",
    "\n",
    "df3[\"doc_text\"] = df3.apply(row_to_doc, axis=1)\n",
    "texts = df3[\"doc_text\"].astype(str).tolist()\n",
    "\n",
    "configs = [\n",
    "    {\"chunk_size\": 350, \"chunk_overlap\": 50},\n",
    "    {\"chunk_size\": 700, \"chunk_overlap\": 100},\n",
    "    {\"chunk_size\": 1000, \"chunk_overlap\": 150},\n",
    "]\n",
    "\n",
    "for cfg in configs:\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=cfg[\"chunk_size\"],\n",
    "        chunk_overlap=cfg[\"chunk_overlap\"],\n",
    "        separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"],\n",
    "    )\n",
    "    total_chunks = sum(len(splitter.split_text(t)) for t in texts)\n",
    "    print(cfg, \"avg_chunks/row:\", round(total_chunks/len(texts), 3), \"total_chunks:\", total_chunks)"
   ],
   "id": "e2e6b51bc3b4a4af",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'chunk_size': 350, 'chunk_overlap': 50} avg_chunks/row: 2.11 total_chunks: 2545\n",
      "{'chunk_size': 700, 'chunk_overlap': 100} avg_chunks/row: 1.004 total_chunks: 1211\n",
      "{'chunk_size': 1000, 'chunk_overlap': 150} avg_chunks/row: 1.0 total_chunks: 1206\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 5) Show 1 example row → chunks (human sanity)",
   "id": "7ec2095c136be8b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-04T10:22:21.959403Z",
     "start_time": "2026-02-04T10:22:21.956836Z"
    }
   },
   "cell_type": "code",
   "source": [
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=700,\n",
    "    chunk_overlap=100,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"],\n",
    ")\n",
    "\n",
    "example_text = texts[0]\n",
    "chunks = splitter.split_text(example_text)\n",
    "\n",
    "print(\"Example doc length:\", len(example_text))\n",
    "print(\"Num chunks:\", len(chunks))\n",
    "print(\"\\n--- Chunk #1 ---\\n\", chunks[0][:800])\n",
    "if len(chunks) > 1:\n",
    "    print(\"\\n--- Chunk #2 ---\\n\", chunks[1][:800])"
   ],
   "id": "e6c28754201bb7fa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example doc length: 610\n",
      "Num chunks: 1\n",
      "\n",
      "--- Chunk #1 ---\n",
      " TITLE: Client Support Specialist at Clipboard Health\n",
      "SOURCE: RemoteJob\n",
      "DATE: 2026-01-16 00:00:00\n",
      "DESCRIPTION: About the Role\n",
      " \n",
      "Clipboard Health is looking for highly motivated, customer-focused individuals to join our team as B2B Support Specialists (Workplace Support Agents). This is not a traditional call center role—you will be the frontline specialist for our most valuable business clients, our workplace customers. Your job is to proactively solve client issues, prevent churn, and ensure a seamless experience for our customers. \n",
      "This is primarily a voice-based role, with additional responsibilities\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 6) Embedding sanity check (OpenAI, prints dim)",
   "id": "5ea14747c33091"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-04T10:23:55.574083Z",
     "start_time": "2026-02-04T10:23:52.152013Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "BASE_DIR = Path(__file__).resolve().parent if \"__file__\" in globals() else Path.cwd()\n",
    "PROJECT_ROOT = BASE_DIR.parent\n",
    "\n",
    "api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "if not api_key:\n",
    "    api_key = (PROJECT_ROOT / \"openai.txt\").read_text().strip()\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "EMBED_MODEL = \"text-embedding-3-small\"\n",
    "probe_texts = [\n",
    "    \"reinforcement learning agent roles\",\n",
    "    \"multi-agent orchestration with tool calling\",\n",
    "    \"customer support specialist\",\n",
    "]\n",
    "\n",
    "resp = client.embeddings.create(model=EMBED_MODEL, input=probe_texts)\n",
    "vecs = [d.embedding for d in resp.data]\n",
    "\n",
    "print(\"Model:\", EMBED_MODEL)\n",
    "print(\"Num vectors:\", len(vecs))\n",
    "print(\"Dim:\", len(vecs[0]))"
   ],
   "id": "9d680e53a22f04",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: text-embedding-3-small\n",
      "Num vectors: 3\n",
      "Dim: 1536\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 7) Retrieval example from your persisted ChromaDB",
   "id": "eed3961f7b3d5287"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-04T10:35:04.654491Z",
     "start_time": "2026-02-04T10:35:02.237801Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# notebook → project root\n",
    "PROJECT_ROOT = Path.cwd().parent\n",
    "DB_DIR = PROJECT_ROOT / \"chroma_db\"\n",
    "COLLECTION_NAME = \"ai_agents_jobs_2026\"\n",
    "TOP_K = 5\n",
    "\n",
    "assert DB_DIR.exists(), f\"Missing {DB_DIR}. Run ingestion first.\"\n",
    "print(\"Using DB_DIR:\", DB_DIR)\n",
    "\n",
    "# Load key (env var OR openai.txt)\n",
    "api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "if not api_key:\n",
    "    key_path = PROJECT_ROOT / \"openai.txt\"\n",
    "    if key_path.exists():\n",
    "        api_key = key_path.read_text().strip()\n",
    "if not api_key:\n",
    "    raise RuntimeError(\"Missing OPENAI_API_KEY\")\n",
    "os.environ[\"OPENAI_API_KEY\"] = api_key\n",
    "\n",
    "# Load vectorstore (uses same embedding model as ingestion)\n",
    "emb = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "vs = Chroma(\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    persist_directory=str(DB_DIR),\n",
    "    embedding_function=emb,\n",
    ")\n",
    "\n",
    "print(\"Count:\", vs._collection.count())\n",
    "\n",
    "queries = [\n",
    "    \"multi-agent orchestration using LangGraph\",\n",
    "    \"tool-calling agents in production systems\",\n",
    "    \"reinforcement learning agent roles in finance\",\n",
    "]\n",
    "\n",
    "for q in queries:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"Query:\", q)\n",
    "    docs = vs.similarity_search(q, k=TOP_K)\n",
    "    for i, d in enumerate(docs, start=1):\n",
    "        md = d.metadata or {}\n",
    "        snippet = (d.page_content or \"\").replace(\"\\n\", \" \")[:200]\n",
    "        print(f\"\\n  Rank {i} | {md.get('title','')}\")\n",
    "        print(f\"  {md.get('source','')} | {md.get('date','')}\")\n",
    "        print(\"  Link:\", md.get(\"link\",\"\"))\n",
    "        print(\"  Snippet:\", snippet, \"...\")"
   ],
   "id": "dd7ee8669fced937",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using DB_DIR: /Users/demidiao/PycharmProjects/generative-ai-rag/chroma_db\n",
      "Count: 652\n",
      "\n",
      "================================================================================\n",
      "Query: multi-agent orchestration using LangGraph\n",
      "\n",
      "  Rank 1 | Learning Latency-Aware Orchestration for Parallel Multi-Agent Systems\n",
      "  ArXiv | 2026-01-15\n",
      "  Link: http://arxiv.org/abs/2601.10560v1\n",
      "  Snippet: TITLE: Learning Latency-Aware Orchestration for Parallel Multi-Agent Systems SOURCE: ArXiv DATE: 2026-01-15 DESCRIPTION: Multi-agent systems (MAS) enable complex reasoning by coordinating multiple age ...\n",
      "\n",
      "  Rank 2 | SC-MAS: Constructing Cost-Efficient Multi-Agent Systems with Edge-Level Heterogeneous Collaboration\n",
      "  ArXiv | 2026-01-14\n",
      "  Link: http://arxiv.org/abs/2601.09434v1\n",
      "  Snippet: TITLE: SC-MAS: Constructing Cost-Efficient Multi-Agent Systems with Edge-Level Heterogeneous Collaboration SOURCE: ArXiv DATE: 2026-01-14 DESCRIPTION: Large Language Model (LLM)-based Multi-Agent Syst ...\n",
      "\n",
      "  Rank 3 | Beyond Rule-Based Workflows: An Information-Flow-Orchestrated Multi-Agents Paradigm via Agent-to-Agent Communication from CORAL\n",
      "  ArXiv | 2026-01-14\n",
      "  Link: http://arxiv.org/abs/2601.09883v1\n",
      "  Snippet: TITLE: Beyond Rule-Based Workflows: An Information-Flow-Orchestrated Multi-Agents Paradigm via Agent-to-Agent Communication from CORAL SOURCE: ArXiv DATE: 2026-01-14 DESCRIPTION: Most existing Large L ...\n",
      "\n",
      "  Rank 4 | MAXS: Meta-Adaptive Exploration with LLM Agents\n",
      "  ArXiv | 2026-01-14\n",
      "  Link: http://arxiv.org/abs/2601.09259v1\n",
      "  Snippet: TITLE: MAXS: Meta-Adaptive Exploration with LLM Agents SOURCE: ArXiv DATE: 2026-01-14 DESCRIPTION: Large Language Model (LLM) Agents exhibit inherent reasoning abilities through the collaboration of m ...\n",
      "\n",
      "  Rank 5 | Show HN: Agent Actors – Plan-Do-Check-Adjust with Parallelized LLM Agent Trees\n",
      "  HackerNews | 2023-04-26\n",
      "  Link: https://github.com/shaman-ai/agent-actors\n",
      "  Snippet: TITLE: Show HN: Agent Actors – Plan-Do-Check-Adjust with Parallelized LLM Agent Trees SOURCE: HackerNews DATE: 2023-04-26 DESCRIPTION: Hey HN! Been working on this library for architecting stateful LL ...\n",
      "\n",
      "================================================================================\n",
      "Query: tool-calling agents in production systems\n",
      "\n",
      "  Rank 1 | ToolSafe: Enhancing Tool Invocation Safety of LLM-based agents via Proactive Step-level Guardrail and Feedback\n",
      "  ArXiv | 2026-01-15\n",
      "  Link: http://arxiv.org/abs/2601.10156v1\n",
      "  Snippet: TITLE: ToolSafe: Enhancing Tool Invocation Safety of LLM-based agents via Proactive Step-level Guardrail and Feedback SOURCE: ArXiv DATE: 2026-01-15 DESCRIPTION: While LLM-based agents can interact wi ...\n",
      "\n",
      "  Rank 2 | Blue Teaming Function-Calling Agents\n",
      "  ArXiv | 2026-01-14\n",
      "  Link: http://arxiv.org/abs/2601.09292v1\n",
      "  Snippet: TITLE: Blue Teaming Function-Calling Agents SOURCE: ArXiv DATE: 2026-01-14 DESCRIPTION: We present an experimental evaluation that assesses the robustness of four open source LLMs claiming function-ca ...\n",
      "\n",
      "  Rank 3 | Show HN: Agent Actors – Plan-Do-Check-Adjust with Parallelized LLM Agent Trees\n",
      "  HackerNews | 2023-04-26\n",
      "  Link: https://github.com/shaman-ai/agent-actors\n",
      "  Snippet: TITLE: Show HN: Agent Actors – Plan-Do-Check-Adjust with Parallelized LLM Agent Trees SOURCE: HackerNews DATE: 2023-04-26 DESCRIPTION: Hey HN! Been working on this library for architecting stateful LL ...\n",
      "\n",
      "  Rank 4 | Ask HN: Who is deploying LLM agents in production?\n",
      "  HackerNews | 2023-08-14\n",
      "  Link: https://news.ycombinator.com/item?id=37120434\n",
      "  Snippet: TITLE: Ask HN: Who is deploying LLM agents in production? SOURCE: HackerNews DATE: 2023-08-14 DESCRIPTION: There are quite some popular OS LLM agents like auto-gpt but they are still &quot;experiments ...\n",
      "\n",
      "  Rank 5 | Ask HN: AI agents look great in demos, but how are people using it?\n",
      "  HackerNews | 2025-12-15\n",
      "  Link: https://news.ycombinator.com/item?id=46275319\n",
      "  Snippet: TITLE: Ask HN: AI agents look great in demos, but how are people using it? SOURCE: HackerNews DATE: 2025-12-15 DESCRIPTION: I lead digital and brand at a major CPG company, and we recently used agenti ...\n",
      "\n",
      "================================================================================\n",
      "Query: reinforcement learning agent roles in finance\n",
      "\n",
      "  Rank 1 | UserLM-R1: Modeling Human Reasoning in User Language Models with Multi-Reward Reinforcement Learning\n",
      "  ArXiv | 2026-01-14\n",
      "  Link: http://arxiv.org/abs/2601.09215v1\n",
      "  Snippet: TITLE: UserLM-R1: Modeling Human Reasoning in User Language Models with Multi-Reward Reinforcement Learning SOURCE: ArXiv DATE: 2026-01-14 DESCRIPTION: User simulators serve as the critical interactiv ...\n",
      "\n",
      "  Rank 2 | Collaborative Multi-Agent Test-Time Reinforcement Learning for Reasoning\n",
      "  ArXiv | 2026-01-14\n",
      "  Link: http://arxiv.org/abs/2601.09667v2\n",
      "  Snippet: TITLE: Collaborative Multi-Agent Test-Time Reinforcement Learning for Reasoning SOURCE: ArXiv DATE: 2026-01-14 DESCRIPTION: Multi-agent systems have evolved into practical LLM-driven collaborators for ...\n",
      "\n",
      "  Rank 3 | When Personas Override Payoffs: Role Identity Bias in Multi-Agent LLM Decision-Making\n",
      "  ArXiv | 2026-01-15\n",
      "  Link: http://arxiv.org/abs/2601.10102v1\n",
      "  Snippet: TITLE: When Personas Override Payoffs: Role Identity Bias in Multi-Agent LLM Decision-Making SOURCE: ArXiv DATE: 2026-01-15 DESCRIPTION: Large language models are increasingly deployed in multi-agent  ...\n",
      "\n",
      "  Rank 4 | Show HN: Autonomous AI agents that monitor the stock market for you\n",
      "  HackerNews | 2024-12-16\n",
      "  Link: https://decodeinvesting.com/chat?agents=lima\n",
      "  Snippet: TITLE: Show HN: Autonomous AI agents that monitor the stock market for you SOURCE: HackerNews DATE: 2024-12-16 DESCRIPTION: We created autonomous AI Agents that monitor the stock market for you while  ...\n",
      "\n",
      "  Rank 5 | Ask HN: Thoughts on an AI agent that must make money to stay alive?\n",
      "  HackerNews | 2025-04-24\n",
      "  Link: https://news.ycombinator.com/item?id=43778708\n",
      "  Snippet: TITLE: Ask HN: Thoughts on an AI agent that must make money to stay alive? SOURCE: HackerNews DATE: 2025-04-24 DESCRIPTION: I’ve been thinking about a new kind of AI experiment: what if we created a l ...\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Sample Outputs — Generative AI RAG (Part 2)\n",
    "\n",
    "This section records outputs for:\n",
    "- Original LLM (no RAG)\n",
    "- Simple RAG\n",
    "- RAG + HyDE\n",
    "- RAG + Reranking\n",
    "\n",
    "Evaluated on one question/prompt."
   ],
   "id": "c110757b55df99a9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-04T10:35:29.331880Z",
     "start_time": "2026-02-04T10:35:29.305430Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List, Optional\n",
    "\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ],
   "id": "eca12f5aa1848e7a",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-04T10:35:30.897235Z",
     "start_time": "2026-02-04T10:35:30.894406Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# -----------------------\n",
    "# Config\n",
    "# -----------------------\n",
    "BASE_DIR = Path(__file__).resolve().parent if \"__file__\" in globals() else Path.cwd()\n",
    "PROJECT_ROOT = BASE_DIR.parent\n",
    "DB_DIR = PROJECT_ROOT / \"chroma_db\"\n",
    "\n",
    "COLLECTION_NAME = \"ai_agents_jobs_2026\"\n",
    "EMBED_MODEL = \"text-embedding-3-small\"\n",
    "LLM_MODEL = \"gpt-4o-mini\"\n",
    "\n",
    "TOP_K = 5\n",
    "CANDIDATES_K = 40\n",
    "RERANK_SNIPPET_CHARS = 350"
   ],
   "id": "afa4dffcb5d5f765",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-04T10:35:32.423168Z",
     "start_time": "2026-02-04T10:35:32.419509Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# -----------------------\n",
    "# OpenAI key\n",
    "# -----------------------\n",
    "api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "if not api_key:\n",
    "    # optional convenience for course environment; ensure openai.txt is gitignored\n",
    "    key_path = PROJECT_ROOT / \"openai.txt\"\n",
    "    if key_path.exists():\n",
    "        api_key = key_path.read_text().strip()\n",
    "\n",
    "if not api_key:\n",
    "    raise RuntimeError(\"Missing OPENAI_API_KEY. Please export it in your environment.\")\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = api_key"
   ],
   "id": "39509215dab9006c",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-04T10:35:34.533498Z",
     "start_time": "2026-02-04T10:35:34.490523Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# -----------------------\n",
    "# Vector store\n",
    "# -----------------------\n",
    "if not DB_DIR.exists():\n",
    "    raise RuntimeError(f\"Missing {DB_DIR}. Run Part 1 ingestion to build the Chroma DB first.\")\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=EMBED_MODEL)\n",
    "vectorstore = Chroma(\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    persist_directory=str(DB_DIR),\n",
    "    embedding_function=embeddings,\n",
    ")\n",
    "\n",
    "# Note: _collection is internal; useful for debugging in coursework\n",
    "try:\n",
    "    count = vectorstore._collection.count()\n",
    "except Exception:\n",
    "    count = \"unknown\"\n",
    "\n",
    "print(\"Persist dir:\", DB_DIR)\n",
    "print(\"Collection:\", COLLECTION_NAME)\n",
    "print(\"Count:\", count)"
   ],
   "id": "f81db96e01d0595",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Persist dir: /Users/demidiao/PycharmProjects/generative-ai-rag/chroma_db\n",
      "Collection: ai_agents_jobs_2026\n",
      "Count: 652\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-04T10:35:39.398409Z",
     "start_time": "2026-02-04T10:35:39.393041Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# -----------------------\n",
    "# Retrieval helpers\n",
    "# -----------------------\n",
    "def retrieve_simple(query: str, k: int = TOP_K):\n",
    "    \"\"\"Simple RAG baseline: plain similarity search.\"\"\"\n",
    "    return vectorstore.similarity_search(query, k=k)\n",
    "\n",
    "def retrieve_candidates(query: str, k: int = CANDIDATES_K):\n",
    "    \"\"\"Candidate pool for HyDE / rerank (can be MMR for diversity).\"\"\"\n",
    "    return vectorstore.max_marginal_relevance_search(\n",
    "        query,\n",
    "        k=k,\n",
    "        fetch_k=max(120, k * 3),\n",
    "        lambda_mult=0.6\n",
    "    )\n",
    "\n",
    "def format_docs(docs: List) -> str:\n",
    "    parts = []\n",
    "    for i, d in enumerate(docs, start=1):\n",
    "        md = d.metadata or {}\n",
    "        header = (\n",
    "            f\"[{i}] title={md.get('title','')} | source={md.get('source','')} | \"\n",
    "            f\"date={md.get('date','')} | link={md.get('link','')}\"\n",
    "        )\n",
    "        parts.append(header + \"\\n\" + (d.page_content or \"\"))\n",
    "    return \"\\n\\n\".join(parts)\n",
    "\n",
    "def format_docs_for_rerank(docs: List, snippet_chars: int = RERANK_SNIPPET_CHARS) -> str:\n",
    "    parts = []\n",
    "    for i, d in enumerate(docs, start=1):\n",
    "        md = d.metadata or {}\n",
    "        text = (d.page_content or \"\").replace(\"\\n\", \" \").strip()[:snippet_chars]\n",
    "        header = (\n",
    "            f\"[{i}] title={md.get('title','')} | source={md.get('source','')} | \"\n",
    "            f\"date={md.get('date','')} | link={md.get('link','')}\"\n",
    "        )\n",
    "        parts.append(header + \"\\n\" + text)\n",
    "    return \"\\n\\n\".join(parts)"
   ],
   "id": "62d4e59701871a56",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-04T10:35:41.419240Z",
     "start_time": "2026-02-04T10:35:41.377902Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# -----------------------\n",
    "# LLMs\n",
    "# -----------------------\n",
    "parser = StrOutputParser()\n",
    "llm = ChatOpenAI(model=LLM_MODEL, temperature=0.2, max_tokens=650)\n",
    "hyde_llm = ChatOpenAI(model=LLM_MODEL, temperature=0.0, max_tokens=250)\n",
    "rerank_llm = ChatOpenAI(model=LLM_MODEL, temperature=0.0, max_tokens=120)"
   ],
   "id": "20e3ba3411cd0d8",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-04T10:35:42.805086Z",
     "start_time": "2026-02-04T10:35:42.800648Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# -----------------------\n",
    "# Prompts\n",
    "# -----------------------\n",
    "base_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Answer the user's question as best you can. If you are unsure, say you don't know.\"),\n",
    "    (\"user\", \"{question}\")\n",
    "])\n",
    "\n",
    "rag_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\n",
    "     \"Answer using ONLY the provided context. \"\n",
    "     \"When you reference a passage, cite the SAME bracket number as in the context header (e.g., [1]). \"\n",
    "     \"If the context does not contain the answer, reply exactly: \"\n",
    "     \"'I don't know based on the provided context.'\"),\n",
    "    (\"user\", \"Question: {question}\\n\\nContext:\\n{context}\\n\\nAnswer:\")\n",
    "])\n",
    "\n",
    "hyde_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\n",
    "     \"Write a short hypothetical answer (5-8 sentences max) that would likely appear in the target documents. \"\n",
    "     \"Focus on key terms that should match relevant documents. \"\n",
    "     \"Do not cite sources. Do not invent specific tool names unless implied by the question.\"),\n",
    "    (\"user\", \"Question: {question}\\nHypothetical answer:\")\n",
    "])\n",
    "\n",
    "rerank_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\n",
    "     \"You are reranking retrieved passages for relevance to the question. \"\n",
    "     \"Return ONLY a comma-separated list of the best passage numbers, in order, like: 3,1,5,2,4. \"\n",
    "     \"Return exactly {top_k} numbers.\"),\n",
    "    (\"user\", \"Question: {question}\\n\\nPassages:\\n{context}\\n\\nBest {top_k} passage numbers:\")\n",
    "])"
   ],
   "id": "d3b7f7d6e69ba156",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-04T10:35:45.007213Z",
     "start_time": "2026-02-04T10:35:44.999783Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# -----------------------\n",
    "# Methods\n",
    "# -----------------------\n",
    "def answer_no_rag(question: str) -> str:\n",
    "    return (base_prompt | llm | parser).invoke({\"question\": question})\n",
    "\n",
    "def answer_simple_rag(question: str) -> str:\n",
    "    docs = retrieve_simple(question, k=TOP_K)\n",
    "    context = format_docs(docs)\n",
    "    return (rag_prompt | llm | parser).invoke({\"question\": question, \"context\": context})\n",
    "\n",
    "def answer_hyde_rag(question: str) -> str:\n",
    "    hypothetical = (hyde_prompt | hyde_llm | parser).invoke({\"question\": question})\n",
    "    candidates = retrieve_candidates(hypothetical, k=CANDIDATES_K)\n",
    "    docs = candidates[:TOP_K]\n",
    "    context = format_docs(docs)\n",
    "    return (rag_prompt | llm | parser).invoke({\"question\": question, \"context\": context})\n",
    "\n",
    "def parse_rerank_order(order: str, max_n: int, top_k: int) -> List[int]:\n",
    "    try:\n",
    "        nums = [int(x.strip()) for x in order.split(\",\")]\n",
    "        nums = [n for n in nums if 1 <= n <= max_n]\n",
    "        seen, cleaned = set(), []\n",
    "        for n in nums:\n",
    "            if n not in seen:\n",
    "                cleaned.append(n)\n",
    "                seen.add(n)\n",
    "        cleaned = cleaned[:top_k]\n",
    "        if len(cleaned) < top_k:\n",
    "            cleaned.extend([n for n in range(1, max_n + 1) if n not in seen][: (top_k - len(cleaned))])\n",
    "        return [n - 1 for n in cleaned]\n",
    "    except Exception:\n",
    "        return list(range(min(top_k, max_n)))\n",
    "\n",
    "def answer_rerank_rag(question: str) -> str:\n",
    "    candidates = retrieve_candidates(question, k=CANDIDATES_K)\n",
    "    if not candidates:\n",
    "        return \"I don't know based on the provided context.\"\n",
    "\n",
    "    context_all = format_docs_for_rerank(candidates, snippet_chars=RERANK_SNIPPET_CHARS)\n",
    "    order = (rerank_prompt | rerank_llm | parser).invoke({\n",
    "        \"question\": question,\n",
    "        \"context\": context_all,\n",
    "        \"top_k\": TOP_K\n",
    "    })\n",
    "\n",
    "    idxs = parse_rerank_order(order, max_n=len(candidates), top_k=TOP_K)\n",
    "    chosen = [candidates[i] for i in idxs]\n",
    "    context = format_docs(chosen)\n",
    "    return (rag_prompt | llm | parser).invoke({\"question\": question, \"context\": context})"
   ],
   "id": "e7b3d75c5391e67a",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-04T10:37:52.894875Z",
     "start_time": "2026-02-04T10:35:47.350885Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if __name__ == \"__main__\":\n",
    "    questions = [\n",
    "        \"From the dataset, list 3 frameworks/tools used for multi-agent orchestration. For each, provide one source + date and a one-sentence use case.\",\n",
    "        \"Find 3 sources about deploying LLM agents in production. What constraints/requirements do they mention (latency, safety, monitoring, access control, tool calling)? Provide title + date + link.\",\n",
    "        \"Identify two sources that discuss tool-integrated reasoning / tool-calling. What tasks do they target, and what is the core idea? Include source + date + link.\",\n",
    "        \"In 2026-dated sources, what themes appear around evaluation, reliability, monitoring, or safety of agents? Give 2–3 themes and cite at least 2 sources.\",\n",
    "        \"Which sources mention LoRA (parameter-efficient fine-tuning), and what are they using it for? Provide date + link for each.\",\n",
    "    ]\n",
    "\n",
    "    for i, q in enumerate(questions, start=1):\n",
    "        print(\"\\n\" + \"=\" * 100)\n",
    "        print(f\"Q{i}: {q}\")\n",
    "\n",
    "        print(\"\\n--- Original LLM (no RAG) ---\")\n",
    "        print(answer_no_rag(q))\n",
    "\n",
    "        print(\"\\n--- Simple RAG ---\")\n",
    "        print(answer_simple_rag(q))\n",
    "\n",
    "        print(\"\\n--- RAG + HyDE ---\")\n",
    "        print(answer_hyde_rag(q))\n",
    "\n",
    "        print(\"\\n--- RAG + Reranking ---\")\n",
    "        print(answer_rerank_rag(q))"
   ],
   "id": "455205c29767c134",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "Q1: From the dataset, list 3 frameworks/tools used for multi-agent orchestration. For each, provide one source + date and a one-sentence use case.\n",
      "\n",
      "--- Original LLM (no RAG) ---\n",
      "Here are three frameworks/tools used for multi-agent orchestration, along with a source and a brief use case for each:\n",
      "\n",
      "1. **JADE (Java Agent Development Framework)**\n",
      "   - **Source**: Bellifemine, F., Caire, G., & Greenwood, D. (2007). \"Developing Multi-Agent Systems with JADE.\" *Wiley*.\n",
      "   - **Use Case**: JADE is used to develop complex multi-agent systems where agents can communicate and collaborate to solve problems, such as in automated trading systems.\n",
      "\n",
      "2. **ROS (Robot Operating System)**\n",
      "   - **Source**: Quigley, M., Conley, K., Gerkey, B., Faust, J., Foote, T., & Leibs, J. (2009). \"ROS: An Open-Source Robot Operating System.\" *ICRA Workshop on Open Source Software*.\n",
      "   - **Use Case**: ROS facilitates multi-agent orchestration in robotic systems, enabling multiple robots to coordinate their actions for tasks like search and rescue operations.\n",
      "\n",
      "3. **Apache Kafka**\n",
      "   - **Source**: Kreps, J., Narkhede, N., & Rao, J. (2011). \"Kafka: A Distributed Messaging System for Log Processing.\" *Proceedings of the NetDB*.\n",
      "   - **Use Case**: Apache Kafka is utilized for orchestrating communication between multiple agents in real-time data processing applications, such as monitoring and analyzing streaming data from IoT devices.\n",
      "\n",
      "These frameworks/tools are widely recognized in the field of multi-agent systems and orchestration.\n",
      "\n",
      "--- Simple RAG ---\n",
      "1. **Auto GPT style frameworks**  \n",
      "   - **Source**: HackerNews  \n",
      "   - **Date**: 2025-12-15  \n",
      "   - **Use Case**: Used for agentic automation across a product launch, assisting with content generation and localization.\n",
      "\n",
      "2. **CORAL**  \n",
      "   - **Source**: ArXiv  \n",
      "   - **Date**: 2026-01-14  \n",
      "   - **Use Case**: Implements an information-flow-orchestrated multi-agent paradigm via agent-to-agent communication, moving beyond rule-based workflows.\n",
      "\n",
      "3. **Latency-Aware Orchestration**  \n",
      "   - **Source**: ArXiv  \n",
      "   - **Date**: 2026-01-15  \n",
      "   - **Use Case**: Focuses on optimizing multi-agent systems for scalability and usability in time-sensitive scenarios by controlling latency under parallel execution.\n",
      "\n",
      "--- RAG + HyDE ---\n",
      "1. **JACoB**  \n",
      "   - **Source**: HackerNews  \n",
      "   - **Date**: 2024-05-23  \n",
      "   - **Use Case**: JACoB is designed to streamline the development process by assisting with tasks like code reviews and converting designs to React components.\n",
      "\n",
      "2. **CORAL**  \n",
      "   - **Source**: ArXiv  \n",
      "   - **Date**: 2026-01-14  \n",
      "   - **Use Case**: CORAL proposes an information-flow-orchestrated multi-agent paradigm that enhances agent-to-agent communication beyond traditional rule-based workflows.\n",
      "\n",
      "3. **Agent Contracts**  \n",
      "   - **Source**: ArXiv  \n",
      "   - **Date**: 2026-01-13  \n",
      "   - **Use Case**: The Agent Contracts framework provides a formal mechanism for resource governance in autonomous AI systems, enabling better task allocation and resource management.\n",
      "\n",
      "--- RAG + Reranking ---\n",
      "1. Framework/Tool: Learning Latency-Aware Orchestration  \n",
      "   Source: ArXiv, 2026-01-15  \n",
      "   Use Case: This framework addresses high inference latency in multi-agent systems by optimizing task performance and inference cost under parallel execution scenarios [1].\n",
      "\n",
      "2. Framework/Tool: CORAL  \n",
      "   Source: ArXiv, 2026-01-14  \n",
      "   Use Case: CORAL introduces an information-flow-orchestrated paradigm that enhances agent-to-agent communication beyond traditional rule-based workflows in multi-agent systems [2].\n",
      "\n",
      "3. Framework/Tool: Gambit  \n",
      "   Source: HackerNews, 2026-01-16  \n",
      "   Use Case: Gambit serves as an open-source agent harness that simplifies the orchestration of AI agents by managing tool calling and planning without extensive developer intervention [3].\n",
      "\n",
      "====================================================================================================\n",
      "Q2: Find 3 sources about deploying LLM agents in production. What constraints/requirements do they mention (latency, safety, monitoring, access control, tool calling)? Provide title + date + link.\n",
      "\n",
      "--- Original LLM (no RAG) ---\n",
      "I don't have the ability to browse the internet or access real-time data, so I can't provide specific sources or links. However, I can suggest some general constraints and requirements that are often discussed in the context of deploying large language model (LLM) agents in production:\n",
      "\n",
      "1. **Latency**: Many deployments require low-latency responses to ensure a smooth user experience. This is particularly important in real-time applications.\n",
      "\n",
      "2. **Safety**: Ensuring that the LLM behaves safely and ethically is crucial. This includes preventing harmful outputs and ensuring that the model adheres to guidelines and regulations.\n",
      "\n",
      "3. **Monitoring**: Continuous monitoring of the model's performance and behavior is essential to detect any anomalies or degradation in quality over time.\n",
      "\n",
      "4. **Access Control**: Implementing strict access controls is necessary to protect sensitive data and ensure that only authorized users can interact with the model.\n",
      "\n",
      "5. **Tool Calling**: The ability to integrate and call external tools or APIs is often a requirement for LLMs to perform specific tasks or retrieve up-to-date information.\n",
      "\n",
      "For specific sources, I recommend searching academic databases, tech blogs, or industry reports that focus on AI and machine learning deployment practices.\n",
      "\n",
      "--- Simple RAG ---\n",
      "1. **Title:** ToolSafe: Enhancing Tool Invocation Safety of LLM-based agents via Proactive Step-level Guardrail and Feedback  \n",
      "   **Date:** 2026-01-15  \n",
      "   **Link:** [http://arxiv.org/abs/2601.10156v1](http://arxiv.org/abs/2601.10156v1)  \n",
      "   **Constraints/Requirements Mentioned:** Safety (monitoring step-level tool invocation behaviors in real time), proactive intervention before unsafe execution.\n",
      "\n",
      "2. **Title:** LLM-Based Agentic Systems for Software Engineering: Challenges and Opportunities  \n",
      "   **Date:** 2026-01-14  \n",
      "   **Link:** [http://arxiv.org/abs/2601.09822v1](http://arxiv.org/abs/2601.09822v1)  \n",
      "   **Constraints/Requirements Mentioned:** Not explicitly mentioned in the provided context.\n",
      "\n",
      "3. **Title:** Blue Teaming Function-Calling Agents  \n",
      "   **Date:** 2026-01-14  \n",
      "   **Link:** [http://arxiv.org/abs/2601.09292v1](http://arxiv.org/abs/2601.09292v1)  \n",
      "   **Constraints/Requirements Mentioned:** Safety (models are not safe by default, effectiveness of defenses in real-world scenarios).\n",
      "\n",
      "--- RAG + HyDE ---\n",
      "I don't know based on the provided context.\n",
      "\n",
      "--- RAG + Reranking ---\n",
      "1. **Title:** ToolSafe: Enhancing Tool Invocation Safety of LLM-based agents via Proactive Step-level Guardrail and Feedback  \n",
      "   **Date:** 2026-01-15  \n",
      "   **Link:** [http://arxiv.org/abs/2601.10156v1](http://arxiv.org/abs/2601.10156v1)  \n",
      "   **Constraints/Requirements Mentioned:** Safety in tool invocation, monitoring step-level behaviors in real time.\n",
      "\n",
      "2. **Title:** Blue Teaming Function-Calling Agents  \n",
      "   **Date:** 2026-01-14  \n",
      "   **Link:** [http://arxiv.org/abs/2601.09292v1](http://arxiv.org/abs/2601.09292v1)  \n",
      "   **Constraints/Requirements Mentioned:** Safety, effectiveness of defenses against attacks.\n",
      "\n",
      "3. **Title:** Learning Latency-Aware Orchestration for Parallel Multi-Agent Systems  \n",
      "   **Date:** 2026-01-15  \n",
      "   **Link:** [http://arxiv.org/abs/2601.10560v1](http://arxiv.org/abs/2601.10560v1)  \n",
      "   **Constraints/Requirements Mentioned:** Latency control, scalability, usability in time-sensitive scenarios.\n",
      "\n",
      "====================================================================================================\n",
      "Q3: Identify two sources that discuss tool-integrated reasoning / tool-calling. What tasks do they target, and what is the core idea? Include source + date + link.\n",
      "\n",
      "--- Original LLM (no RAG) ---\n",
      "Here are two sources that discuss tool-integrated reasoning or tool-calling:\n",
      "\n",
      "1. **Source**: \"Tool-Integrated Reasoning: A New Paradigm for AI\"  \n",
      "   **Date**: 2021  \n",
      "   **Link**: [arXiv:2105.12345](https://arxiv.org/abs/2105.12345)  \n",
      "   **Tasks Targeted**: This paper discusses various tasks such as mathematical problem solving, programming, and data analysis.  \n",
      "   **Core Idea**: The core idea is to enhance AI reasoning capabilities by integrating external tools that can perform specific functions, allowing the AI to leverage these tools to solve complex problems that require more than just internal reasoning.\n",
      "\n",
      "2. **Source**: \"The Role of External Tools in AI Reasoning\"  \n",
      "   **Date**: 2022  \n",
      "   **Link**: [arXiv:2201.04567](https://arxiv.org/abs/2201.04567)  \n",
      "   **Tasks Targeted**: This work focuses on tasks like scientific discovery, language translation, and interactive question answering.  \n",
      "   **Core Idea**: The paper emphasizes the importance of using external tools to augment AI's reasoning processes, suggesting that combining internal reasoning with external capabilities can lead to more robust and effective problem-solving strategies.\n",
      "\n",
      "Please verify the links and details as they may change over time.\n",
      "\n",
      "--- Simple RAG ---\n",
      "1. Source: MatchTIR: Fine-Grained Supervision for Tool-Integrated Reasoning via Bipartite Matching | Date: 2026-01-15 | Link: [http://arxiv.org/abs/2601.10712v1](http://arxiv.org/abs/2601.10712v1)  \n",
      "   - Tasks targeted: Complex tasks requiring interleaving reasoning steps with external tool interactions.  \n",
      "   - Core idea: The paper addresses the limitations of existing reinforcement learning methods in credit assignment for tool calls, proposing fine-grained supervision to distinguish effective tool calls from redundant or erroneous ones.\n",
      "\n",
      "2. Source: Improving Chain-of-Thought for Logical Reasoning via Attention-Aware Intervention | Date: 2026-01-14 | Link: [http://arxiv.org/abs/2601.09805v1](http://arxiv.org/abs/2601.09805v1)  \n",
      "   - Tasks targeted: Logical reasoning tasks that require decomposition into subtasks solved through prompts or external resources.  \n",
      "   - Core idea: The paper discusses a non-interactive, end-to-end framework that enhances logical reasoning capabilities by addressing the limitations of interactive and hybrid approaches.\n",
      "\n",
      "--- RAG + HyDE ---\n",
      "1. Source: MatchTIR: Fine-Grained Supervision for Tool-Integrated Reasoning via Bipartite Matching | Date: 2026-01-15 | Link: [http://arxiv.org/abs/2601.10712v1](http://arxiv.org/abs/2601.10712v1)  \n",
      "   - Tasks targeted: Complex tasks requiring interleaving reasoning steps with external tool interactions.  \n",
      "   - Core idea: The paper addresses the limitations of existing reinforcement learning methods in credit assignment for tool calls, proposing a fine-grained supervision approach to distinguish effective tool calls from redundant or erroneous ones.\n",
      "\n",
      "2. Source: Improving Chain-of-Thought for Logical Reasoning via Attention-Aware Intervention | Date: 2026-01-14 | Link: [http://arxiv.org/abs/2601.09805v1](http://arxiv.org/abs/2601.09805v1)  \n",
      "   - Tasks targeted: Logical reasoning tasks that decompose the reasoning process into subtasks solved through prompts or external resources.  \n",
      "   - Core idea: The paper discusses a non-interactive, end-to-end framework that enables reasoning to emerge without the overhead of complex interactive frameworks, focusing on enhancing logical reasoning capabilities.\n",
      "\n",
      "--- RAG + Reranking ---\n",
      "1. **Source**: MatchTIR: Fine-Grained Supervision for Tool-Integrated Reasoning via Bipartite Matching  \n",
      "   **Date**: 2026-01-15  \n",
      "   **Link**: [http://arxiv.org/abs/2601.10712v1](http://arxiv.org/abs/2601.10712v1)  \n",
      "   **Tasks Targeted**: Complex tasks involving interleaving reasoning steps with external tool interactions.  \n",
      "   **Core Idea**: The paper addresses the limitations of existing reinforcement learning methods in credit assignment for tool calls, proposing fine-grained supervision to distinguish effective tool calls from redundant or erroneous ones.\n",
      "\n",
      "2. **Source**: ToolSafe: Enhancing Tool Invocation Safety of LLM-based agents via Proactive Step-level Guardrail and Feedback  \n",
      "   **Date**: 2026-01-15  \n",
      "   **Link**: [http://arxiv.org/abs/2601.10156v1](http://arxiv.org/abs/2601.10156v1)  \n",
      "   **Tasks Targeted**: Step-level tool invocation safety detection in LLM agents.  \n",
      "   **Core Idea**: The work focuses on enhancing safety in tool invocation by monitoring behaviors in real time and proactively intervening before unsafe executions occur, introducing a novel benchmark for this purpose.\n",
      "\n",
      "====================================================================================================\n",
      "Q4: In 2026-dated sources, what themes appear around evaluation, reliability, monitoring, or safety of agents? Give 2–3 themes and cite at least 2 sources.\n",
      "\n",
      "--- Original LLM (no RAG) ---\n",
      "I'm sorry, but I cannot provide information from 2026-dated sources as my training only includes data up to October 2023. I do not have access to future publications or their content. If you have questions about themes related to evaluation, reliability, monitoring, or safety of agents based on existing knowledge, feel free to ask!\n",
      "\n",
      "--- Simple RAG ---\n",
      "In 2026-dated sources, several themes emerge around evaluation, reliability, monitoring, or safety of agents:\n",
      "\n",
      "1. **Automated Evaluation Frameworks**: The development of frameworks like DeepResearchEval and DR-Arena highlights the need for automated systems to evaluate the performance of deep research agents reliably. These frameworks aim to address challenges in existing benchmarks, such as static evaluation dimensions and task generality limitations ([2], [4]).\n",
      "\n",
      "2. **Security Vulnerabilities and Trust**: The study on agent skills emphasizes the security risks associated with AI agents executing skills with implicit trust and minimal vetting. This creates a significant attack surface that needs to be characterized and monitored for safety ([3]).\n",
      "\n",
      "3. **Access Control Policies**: The introduction of AgentGuardian focuses on learning access control policies to govern AI agent behavior, ensuring that agents perform authorized actions and handle inputs appropriately. This is crucial for maintaining system integrity and preventing misuse ([5]).\n",
      "\n",
      "Sources: [2], [3], [4], [5].\n",
      "\n",
      "--- RAG + HyDE ---\n",
      "In 2026-dated sources, several themes around evaluation, reliability, monitoring, or safety of agents emerge:\n",
      "\n",
      "1. **Access Control and Security**: The introduction of frameworks like AgentGuardian emphasizes the importance of enforcing context-aware access-control policies to ensure that AI agents perform only authorized actions and handle inputs appropriately, which is crucial for maintaining system integrity and preventing misuse [3].\n",
      "\n",
      "2. **Vulnerability and Trust Issues**: The study on agent skills highlights significant security vulnerabilities due to the implicit trust and minimal vetting of skills that agents use. This creates a considerable attack surface, indicating a need for better monitoring and evaluation of agent capabilities and their security implications [4].\n",
      "\n",
      "3. **Framework Maturity and Impact on Performance**: The discussion on agentic frameworks suggests that the choice of framework significantly influences the capabilities and reliability of agents, indicating a need for careful evaluation of these frameworks to ensure optimal performance and safety [1].\n",
      "\n",
      "Sources: [1], [3], [4].\n",
      "\n",
      "--- RAG + Reranking ---\n",
      "In 2026-dated sources, the following themes appear around evaluation, reliability, monitoring, or safety of agents:\n",
      "\n",
      "1. **Evaluation Challenges**: The evaluation of deep research systems is highlighted as challenging due to existing benchmarks that require intensive task construction and fail to reliably verify facts. This theme is discussed in the context of the automated framework introduced in DeepResearchEval, which aims to improve agentic evaluation ([1]).\n",
      "\n",
      "2. **Security Vulnerabilities**: The study on agent skills reveals significant security vulnerabilities due to the implicit trust and minimal vetting of modular packages that extend agent capabilities. This creates an uncharacterized attack surface, emphasizing the need for better monitoring and evaluation of security in AI agents ([2]).\n",
      "\n",
      "3. **Access Control Policies**: The introduction of the AgentGuardian framework focuses on learning access control policies to ensure that AI agents perform only authorized actions. This theme underscores the importance of monitoring agent behavior to maintain system integrity and prevent misuse ([3]).\n",
      "\n",
      "Sources:\n",
      "- [1] DeepResearchEval: An Automated Framework for Deep Research Task Construction and Agentic Evaluation\n",
      "- [2] Agent Skills in the Wild: An Empirical Study of Security Vulnerabilities at Scale\n",
      "- [3] AgentGuardian: Learning Access Control Policies to Govern AI Agent Behavior\n",
      "\n",
      "====================================================================================================\n",
      "Q5: Which sources mention LoRA (parameter-efficient fine-tuning), and what are they using it for? Provide date + link for each.\n",
      "\n",
      "--- Original LLM (no RAG) ---\n",
      "LoRA (Low-Rank Adaptation) is a technique for parameter-efficient fine-tuning of large language models. Here are some sources that mention LoRA and its applications:\n",
      "\n",
      "1. **Paper: \"LoRA: Low-Rank Adaptation of Large Language Models\"**\n",
      "   - **Date:** 2021\n",
      "   - **Link:** [LoRA Paper](https://arxiv.org/abs/2106.09685)\n",
      "   - **Usage:** This paper introduces LoRA as a method to fine-tune large language models efficiently by injecting low-rank matrices into each layer of the model, allowing for effective adaptation with significantly fewer trainable parameters.\n",
      "\n",
      "2. **Blog Post: \"Parameter-Efficient Fine-Tuning with LoRA\"**\n",
      "   - **Date:** 2022\n",
      "   - **Link:** [Hugging Face Blog](https://huggingface.co/blog/lora)\n",
      "   - **Usage:** The blog discusses how LoRA can be used to fine-tune models for specific tasks while minimizing the number of parameters that need to be trained, making it suitable for applications in NLP tasks like text classification and summarization.\n",
      "\n",
      "3. **Research Paper: \"Parameter-Efficient Transfer Learning for NLP\"**\n",
      "   - **Date:** 2022\n",
      "   - **Link:** [Research Paper](https://arxiv.org/abs/2202.11206)\n",
      "   - **Usage:** This paper explores various parameter-efficient methods, including LoRA, for transferring knowledge from large pre-trained models to downstream tasks, highlighting its effectiveness in improving performance on specific NLP benchmarks.\n",
      "\n",
      "4. **GitHub Repository: \"LoRA Implementation\"**\n",
      "   - **Date:** 2022\n",
      "   - **Link:** [GitHub - LoRA](https://github.com/microsoft/LoRA)\n",
      "   - **Usage:** This repository provides an implementation of LoRA, showcasing its application in various machine learning frameworks and demonstrating its use in fine-tuning models for tasks such as dialogue generation and sentiment analysis.\n",
      "\n",
      "These sources provide a good overview of LoRA's introduction, its applications in fine-tuning large language models, and practical implementations.\n",
      "\n",
      "--- Simple RAG ---\n",
      "1. **Title**: Instruction Finetuning LLaMA-3-8B Model Using LoRA for Financial Named Entity Recognition  \n",
      "   **Date**: 2026-01-15  \n",
      "   **Link**: [http://arxiv.org/abs/2601.10043v1](http://arxiv.org/abs/2601.10043v1)  \n",
      "   **Usage**: This paper uses LoRA for financial named-entity recognition (NER) by combining instruction fine-tuning and Low-Rank Adaptation (LoRA).\n",
      "\n",
      "2. **Title**: Disentangling Task Conflicts in Multi-Task LoRA via Orthogonal Gradient Projection  \n",
      "   **Date**: 2026-01-14  \n",
      "   **Link**: [http://arxiv.org/abs/2601.09684v1](http://arxiv.org/abs/2601.09684v1)  \n",
      "   **Usage**: This paper discusses the use of LoRA in Multi-Task Learning (MTL) to enable parameter-efficient deployment of Large Language Models (LLMs) by sharing a single adapter across multiple tasks.\n",
      "\n",
      "3. **Title**: Advancing Model Refinement: Muon-Optimized Distillation and Quantization for LLM Deployment  \n",
      "   **Date**: 2026-01-14  \n",
      "   **Link**: [http://arxiv.org/abs/2601.09865v1](http://arxiv.org/abs/2601.09865v1)  \n",
      "   **Usage**: This paper proposes an integrated framework that combines low-rank adaptation (LoRA) with quantization for optimizing Large Language Models (LLMs) for deployment on resource-constrained edge devices.\n",
      "\n",
      "--- RAG + HyDE ---\n",
      "1. **Instruction Finetuning LLaMA-3-8B Model Using LoRA for Financial Named Entity Recognition**  \n",
      "   - Date: 2026-01-15  \n",
      "   - Link: [http://arxiv.org/abs/2601.10043v1](http://arxiv.org/abs/2601.10043v1)  \n",
      "   - Using LoRA for: Financial named-entity recognition (NER).\n",
      "\n",
      "2. **Disentangling Task Conflicts in Multi-Task LoRA via Orthogonal Gradient Projection**  \n",
      "   - Date: 2026-01-14  \n",
      "   - Link: [http://arxiv.org/abs/2601.09684v1](http://arxiv.org/abs/2601.09684v1)  \n",
      "   - Using LoRA for: Multi-task learning with a focus on reducing storage overhead and addressing negative transfer issues.\n",
      "\n",
      "3. **Advancing Model Refinement: Muon-Optimized Distillation and Quantization for LLM Deployment**  \n",
      "   - Date: 2026-01-14  \n",
      "   - Link: [http://arxiv.org/abs/2601.09865v1](http://arxiv.org/abs/2601.09865v1)  \n",
      "   - Using LoRA for: Optimizing large language models for deployment on resource-constrained edge devices.\n",
      "\n",
      "--- RAG + Reranking ---\n",
      "1. **Title:** Instruction Finetuning LLaMA-3-8B Model Using LoRA for Financial Named Entity Recognition  \n",
      "   **Date:** 2026-01-15  \n",
      "   **Link:** [http://arxiv.org/abs/2601.10043v1](http://arxiv.org/abs/2601.10043v1)  \n",
      "   **Use:** This paper applies LoRA to financial named-entity recognition (NER) by combining instruction fine-tuning and Low-Rank Adaptation (LoRA).\n",
      "\n",
      "2. **Title:** Disentangling Task Conflicts in Multi-Task LoRA via Orthogonal Gradient Projection  \n",
      "   **Date:** 2026-01-14  \n",
      "   **Link:** [http://arxiv.org/abs/2601.09684v1](http://arxiv.org/abs/2601.09684v1)  \n",
      "   **Use:** This paper discusses the use of LoRA in Multi-Task Learning (MTL) for parameter-efficient deployment of Large Language Models (LLMs) while addressing issues of negative transfer from conflicting gradient updates.\n"
     ]
    }
   ],
   "execution_count": 27
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
